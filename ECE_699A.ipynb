{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EV8zLy8pxfjV"
   },
   "outputs": [],
   "source": [
    "# importing variables from json:\n",
    "import json\n",
    "with open('input.json', 'r') as f:\n",
    "    input_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEtXF11FvEnM",
    "outputId": "b8b65849-ad61-4a96-c455-743470ad843b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install keras-tuner matplotlib s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "c8wtnUbZvV5T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "id": "mwc0h8ZowjAo",
    "outputId": "c94df679-5e61-45d3-8948-f09570921031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# df = pd.read_csv(input_data['dataset_path1'])\n",
    "df = pd.read_csv(input_data['dataset_path2'])\n",
    "\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nDataset shape:\", df.shape)\n",
    "print(\"\\nBasic statistics:\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"\\nBasic Info:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "id": "WGnv_lPTwesO",
    "outputId": "37b88d40-515e-4415-d603-782eb27db91b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"\\nCorrelation Matrix:\")\n",
    "corr_matrix = df.corr(numeric_only=True)\n",
    "display(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7XHkclz6wrQW",
    "outputId": "ead86930-7f97-4042-fbe7-2db68be92de0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap(corr_matrix,\n",
    "#             annot=True,\n",
    "#             cmap='coolwarm'\n",
    "#             # fmt=\".2f\",\n",
    "#             # linewidths=.5)\n",
    "# plt.title('Feature Correlation Matrix')\n",
    "# plt.show()\n",
    "\n",
    "plt.figure(figsize=(200, 160))  # Larger figure size\n",
    "sns.heatmap(corr_matrix,\n",
    "            # annot=False,  # Remove annotations to reduce clutter\n",
    "            cmap='coolwarm',\n",
    "            # linewidths=0.1,  # Add thin lines between cells\n",
    "            linecolor='gray')\n",
    "plt.title('Feature Correlation Matrix', fontsize=18)\n",
    "plt.xticks(fontsize=10, rotation=45)  # Adjust x-axis labels\n",
    "plt.yticks(fontsize=10, rotation=0)  # Adjust y-axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "eKugl5ezwtGT",
    "outputId": "6d9543df-ad04-40a8-b637-644699f378bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Correlation Analysis\n",
    "print(\"\\n\\033[1m1. Correlation Analysis\\033[0m\")\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_corr = numeric_df.corr()\n",
    "print(\"\\nPearson Correlation Matrix:\")\n",
    "display(pearson_corr)\n",
    "\n",
    "# Spearman correlation (for non-linear relationships)\n",
    "spearman_corr = numeric_df.corr(method='spearman')\n",
    "print(\"\\nSpearman Rank Correlation Matrix:\")\n",
    "display(spearman_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JShIfhuEzdEV",
    "outputId": "c72fbf37-5de2-484d-f609-b02aae0e78d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Visualization\n",
    "print(\"\\n\\033[1m2. Visual Relationships\\033[0m\")\n",
    "\n",
    "# Heatmap\n",
    "# plt.figure(figsize=(12,8))\n",
    "# sns.heatmap(pearson_corr,\n",
    "#             annot=True,\n",
    "#             cmap='coolwarm',\n",
    "#             vmin=-1, vmax=1,\n",
    "#             mask=np.triu(np.ones_like(pearson_corr)))\n",
    "# plt.title('Correlation Heatmap (Pearson)')\n",
    "# plt.show()\n",
    "\n",
    "# Pairplot for multivariate analysis\n",
    "sns.pairplot(df.sample(min(200, len(df)), random_state=1))\n",
    "plt.suptitle('Pairwise Relationships', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p49gOH3V0GxK"
   },
   "outputs": [],
   "source": [
    "# Statistical Testing\n",
    "print(\"\\n\\033[1m3. Statistical Significance Testing\\033[0m\")\n",
    "\n",
    "# For 2 numeric variables\n",
    "if len(numeric_df.columns) >= 2:\n",
    "    col1, col2 = numeric_df.columns[:2]\n",
    "    r, p = stats.pearsonr(df[col1], df[col2])\n",
    "    print(f\"\\nPearson test between {col1} and {col2}:\")\n",
    "    print(f\"Correlation Coefficient: {r:.3f}\")\n",
    "    print(f\"P-value: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXYzr1t40KZ3"
   },
   "outputs": [],
   "source": [
    "# Categorical Relationships\n",
    "print(\"\\n\\033[1m4. Categorical Analysis\\033[0m\")\n",
    "\n",
    "# For categorical vs numerical\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "if len(categorical_cols) > 0 and len(numeric_df.columns) > 0:\n",
    "    cat_var = categorical_cols[0]\n",
    "    num_var = numeric_df.columns[0]\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.boxplot(x=cat_var, y=num_var, data=df)\n",
    "    plt.title(f'{cat_var} vs {num_var}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQWEoPOV0KR2"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\033[1m5. Advanced Relationships\\033[0m\")\n",
    "\n",
    "# Cross-correlation (lagged relationships)\n",
    "if len(numeric_df.columns) >= 2:\n",
    "    col1, col2 = numeric_df.columns[:2]\n",
    "    cross_corr = np.correlate(df[col1], df[col2], mode='full')\n",
    "    plt.plot(cross_corr)\n",
    "    plt.title(f'Cross-correlation between {col1} and {col2}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qQ8Zfg5U0Z4"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tfX33jtg0WpU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# data = pd.read_csv(input_data['dataset_path1'])\n",
    "data = pd.read_csv(input_data['dataset_path2'])\n",
    "\n",
    "X = data.drop(\"Target\", axis=1).values\n",
    "y = data[\"Target\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize\n",
    "# ### not scale x\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Normalize the target variable\n",
    "y_scaler = StandardScaler()\n",
    "y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_test = y_scaler.transform(y_test.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5lQniFCET9Bw"
   },
   "outputs": [],
   "source": [
    "# # 2. Define the Sparse Autoencoder Model in TensorFlow/Keras\n",
    "# input_dim = X_train.shape[1]\n",
    "# latent_dim = 16  # tune this based on dataset\n",
    "# #### hpo here\n",
    "\n",
    "# #input\n",
    "# input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "# # Encoder\n",
    "# encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "# encoded = layers.BatchNormalization()(encoded)\n",
    "# encoded = layers.Dropout(0.2)(encoded)\n",
    "# latent = layers.Dense(latent_dim, activation='linear',\n",
    "#                       activity_regularizer=regularizers.l1(1e-3))(encoded)\n",
    "# ### ## activity_regularizer, lr, do HPO,\n",
    "# ### start final report\n",
    "\n",
    "# # Decoder\n",
    "# decoded = layers.Dense(64, activation='relu')(latent)\n",
    "# decoded = layers.BatchNormalization()(decoded)\n",
    "# decoded = layers.Dropout(0.2)(decoded)\n",
    "# decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "\n",
    "# autoencoder = keras.Model(inputs=input_layer, outputs=decoded, name=\"sparse_autoencoder\")\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n",
    "# autoencoder.summary()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VRYJqH-TRcPZ",
    "outputId": "cb643b14-c82f-4a96-9361-42691601fa93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# TRY DECREASE LAYER, Python class, plotting everything(loss,lr,acc, etc),\n",
    "# activity_regularizer l1 play with it and relation with lr, powerpoint to it,\n",
    "# Add Sparse\n",
    "\n",
    "def build_model(hp):\n",
    "    input_dim = X_train.shape[1]  # Number of input features\n",
    "\n",
    "    # Hyperparameters to tune:\n",
    "    encoder_units = hp.Int('encoder_units', min_value=8, max_value=256, step=2)\n",
    "    decoder_units = hp.Int('decoder_units', min_value=8, max_value=256, step=2)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.8, step=0.1)\n",
    "    latent_dim = hp.Int('latent_dim', min_value=2, max_value=128, step=2)\n",
    "    l1_reg = hp.Float('l1_reg', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "    # Input layer\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "    # Encoder\n",
    "    x = layers.Dense(encoder_units, activation='relu')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    latent = layers.Dense(latent_dim, activation='linear', activity_regularizer=regularizers.l1(l1_reg), name='latent')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(decoder_units, activation='relu')(latent)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    # Define the autoencoder model\n",
    "    model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"sparse_autoencoder\")\n",
    "\n",
    "    # Compile the model with the tunable learning rate\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse')\n",
    "\n",
    "    return model\n",
    "\n",
    "#2\n",
    "# def build_model(hp):\n",
    "#     input_dim = X_train.shape[1]  # Number of input features\n",
    "\n",
    "#     # Hyperparameters to tune:\n",
    "#     encoder_units_1 = hp.Int('encoder_units_1', min_value=8, max_value=256, step=2)\n",
    "#     encoder_units_2 = hp.Int('encoder_units_2', min_value=8, max_value=256, step=2)\n",
    "#     decoder_units_1 = hp.Int('decoder_units_1', min_value=8, max_value=256, step=2)\n",
    "#     decoder_units_2 = hp.Int('decoder_units_2', min_value=8, max_value=256, step=2)\n",
    "#     dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.8, step=0.1)\n",
    "#     latent_dim = hp.Int('latent_dim', min_value=2, max_value=128, step=2)\n",
    "#     l1_reg = hp.Float('l1_reg', min_value=1e-5, max_value=1e-2, sampling='log')\n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n",
    "\n",
    "#     # Input layer\n",
    "#     input_layer = keras.Input(shape=(input_dim,))\n",
    "\n",
    "#     # Encoder: Increased layers\n",
    "#     x = layers.Dense(encoder_units_1, activation='relu')(input_layer)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     x = layers.Dense(encoder_units_2, activation='relu')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     latent = layers.Dense(latent_dim, activation='linear',\n",
    "#                           activity_regularizer=regularizers.l1(l1_reg),\n",
    "#                           name='latent')(x)\n",
    "\n",
    "#     # Decoder: Increased layers\n",
    "#     x = layers.Dense(decoder_units_1, activation='relu')(latent)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     x = layers.Dense(decoder_units_2, activation='relu')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "#     # Define and compile the autoencoder model\n",
    "#     model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"sparse_autoencoder\")\n",
    "#     model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#                   loss='mse')\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# Set up the Keras Tuner with Random Search (other tuners like BayesianOptimization are also available)\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,           # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,  # Number of models to build for each trial\n",
    "    directory='hpo_dir',\n",
    "    project_name='sparse_autoencoder'\n",
    ")\n",
    "\n",
    "# Print a summary of the search space\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run the hyperparameter search.\n",
    "# Here we train the model as an autoencoder (input is the same as output).\n",
    "tuner.search(X_train, X_train, epochs=500, validation_split=0.1)\n",
    "\n",
    "# Retrieve the best model found by the tuner\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Display a summary of the best model architecture\n",
    "best_model.summary()\n",
    "\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "# Build the autoencoder with the best hyperparameters and train it\n",
    "autoencoder = best_model\n",
    "history = autoencoder.fit(X_train, X_train,\n",
    "                          epochs=500,\n",
    "                          batch_size=8,\n",
    "                          validation_split=0.1,\n",
    "                          callbacks=[early_stop],\n",
    "                          verbose=1)\n",
    "\n",
    "# Confirm that it’s a Sparse Autoencoder (the latent layer includes an L1 regularizer)\n",
    "print(\"Sparse Autoencoder defined with L1 regularization on the latent layer (named 'latent').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXjSJ79xUFvH"
   },
   "outputs": [],
   "source": [
    "# 4. Create the Encoder Model for Feature Extraction\n",
    "# Create a model to extract the latent features\n",
    "encoder = keras.Model(inputs=best_model.input, outputs=best_model.get_layer(\"latent\").output, name=\"encoder\")\n",
    "latent_train = encoder.predict(X_train)\n",
    "latent_test = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5DaKbHnUK2e"
   },
   "outputs": [],
   "source": [
    "# 5. Build and Train a Downstream Regression Model for AGB Prediction\n",
    "def build_regressor_model(hp):\n",
    "    reg_input = keras.Input(shape=(latent_train.shape[1],))\n",
    "\n",
    "    units_1 = hp.Int('units_1', min_value=16, max_value=128, step=16, default=32)\n",
    "    units_2 = hp.Int('units_2', min_value=8, max_value=64, step=8, default=16)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1, default=0.2)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log', default=1e-3)\n",
    "\n",
    "    x = layers.Dense(units_1, activation='relu')(reg_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = layers.Dense(units_2, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    reg_output = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    regressor = keras.Model(inputs=reg_input, outputs=reg_output, name=\"regressor\")\n",
    "\n",
    "    regressor.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss='mse', metrics=['mae'])\n",
    "    return regressor\n",
    "\n",
    "#2\n",
    "# def build_regressor_model(hp):\n",
    "#     reg_input = keras.Input(shape=(latent_train.shape[1],))\n",
    "\n",
    "#     # Hyperparameters for layer sizes\n",
    "#     units_1 = hp.Int('units_1', min_value=32, max_value=256, step=32, default=64)\n",
    "#     units_2 = hp.Int('units_2', min_value=16, max_value=128, step=16, default=32)\n",
    "#     units_3 = hp.Int('units_3', min_value=8, max_value=64, step=8, default=16)\n",
    "#     # Optionally add even another layer:\n",
    "#     # units_4 = hp.Int('units_4', min_value=8, max_value=64, step=8, default=16)\n",
    "\n",
    "#     dropout_rate = hp.Float('dropout_rate', min_value=0.0, max_value=0.5, step=0.1, default=0.2)\n",
    "#     learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log', default=1e-3)\n",
    "\n",
    "#     # First hidden layer\n",
    "#     x = layers.Dense(units_1, activation='relu')(reg_input)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     # Second hidden layer\n",
    "#     x = layers.Dense(units_2, activation='relu')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     # Third hidden layer (new layer)\n",
    "#     x = layers.Dense(units_3, activation='relu')(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "#     reg_output = layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "#     regressor = keras.Model(inputs=reg_input, outputs=reg_output, name=\"regressor\")\n",
    "\n",
    "#     regressor.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "#                       loss='mse', metrics=['mae'])\n",
    "#     return regressor\n",
    "\n",
    "\n",
    "# Then set up the tuner:\n",
    "tuner_regressor = kt.RandomSearch(\n",
    "    build_regressor_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=50,\n",
    "    executions_per_trial=1,\n",
    "    directory='hpo_regressor_dir2',\n",
    "    project_name='regressor_agb'\n",
    ")\n",
    "\n",
    "tuner_regressor.search(latent_train, y_train, epochs=300, validation_split=0.1)\n",
    "\n",
    "best_regressor = tuner_regressor.get_best_models(num_models=1)[0]\n",
    "best_regressor.summary()\n",
    "\n",
    "\n",
    "reg_epochs = 500\n",
    "best_regressor.fit(latent_train, y_train,\n",
    "              epochs=reg_epochs,\n",
    "              batch_size=8,\n",
    "              validation_split=0.1,\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0dk24GNUKxi"
   },
   "outputs": [],
   "source": [
    "y_pred_scaled = best_regressor.predict(latent_test).flatten()\n",
    "# Inverse transform predictions and true values to get original scale\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_inv = y_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "mse_value = mean_squared_error(y_test_inv, y_pred)\n",
    "r2 = r2_score(y_test_inv, y_pred)\n",
    "print(f\"Test MSE for AGB prediction: {mse_value:.4f}\")\n",
    "print(f\"Test R^2 for AGB prediction: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JLPMaC7Kafcg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaKkGT3rUpnd"
   },
   "outputs": [],
   "source": [
    "# make graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTPctvMgagAO"
   },
   "outputs": [],
   "source": [
    "# Plot and save the Autoencoder training loss history\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.title('Autoencoder Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('autoencoder_loss_history.png')\n",
    "plt.show()\n",
    "print(\"Autoencoder loss history plot saved as 'autoencoder_loss_history.png'.\")\n",
    "\n",
    "# Plot and save the predicted vs. actual AGB scatter plot for the regressor\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(y_test_inv, y_pred, color='blue', alpha=0.6, label='Predictions')\n",
    "plt.plot([min(y_test_inv), max(y_test_inv)], [min(y_test_inv), max(y_test_inv)], 'r--', label='Ideal Fit')\n",
    "plt.xlabel('Actual AGB')\n",
    "plt.ylabel('Predicted AGB')\n",
    "plt.title(f'Predicted vs Actual AGB (R^2 = {r2:.2f})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('predicted_vs_actual.png')\n",
    "plt.show()\n",
    "print(\"Predicted vs Actual AGB plot saved as 'predicted_vs_actual.png'.\")\n",
    "\n",
    "# Residual plot\n",
    "residuals = y_test_inv - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True, scatter_kws={\"s\": 50, \"alpha\": 0.5}, line_kws={\"color\": \"red\", \"lw\": 2})\n",
    "plt.xlabel('Predicted AGB')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs. Predicted Above Ground Biomass')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Q-Q plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# # Learning curve\n",
    "# train_sizes, train_scores, val_scores = learning_curve(\n",
    "#     best_regressor, latent_train, y_train, cv=5, scoring='neg_mean_squared_error',\n",
    "#     train_sizes=np.linspace(0.1, 1.0, 10)\n",
    "# )\n",
    "\n",
    "# train_scores_mean = -train_scores.mean(axis=1)\n",
    "# val_scores_mean = -val_scores.mean(axis=1)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training error')\n",
    "# plt.plot(train_sizes, val_scores_mean, 'o-', color='red', label='Validation error')\n",
    "# plt.xlabel('Training Set Size')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.title('Learning Curve')\n",
    "# plt.legend(loc='best')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Regression metrics\n",
    "mae = mean_absolute_error(y_test_inv, y_pred)\n",
    "mse = mean_squared_error(y_test_inv, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_inv, y_pred)\n",
    "\n",
    "print(f'Mean Absolute Error (MAE): {mae:.2f}')\n",
    "print(f'Mean Squared Error (MSE): {mse:.2f}')\n",
    "print(f'Root Mean Squared Error (RMSE): {rmse:.2f}')\n",
    "print(f'R-squared (R2): {r2:.2f}')\n",
    "\n",
    "# Feature importance (example for tree-based models)\n",
    "# importances = best_regressor.feature_importances_\n",
    "# feature_names = data.columns[:-1]  # Assuming the last column is the target\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=importances, y=feature_names)\n",
    "# plt.xlabel('Importance')\n",
    "# plt.ylabel('Feature')\n",
    "# plt.title('Feature Importance')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbRVfe7haf2l"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.inspection import permutation_importance\n",
    "import numpy as np\n",
    "\n",
    "# 1. Training History Visualization\n",
    "def plot_training_history(history, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{title} - Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend()\n",
    "\n",
    "    if 'mae' in history.history:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['mae'], label='Train MAE')\n",
    "        plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "        plt.title(f'{title} - MAE')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}_training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot autoencoder training history\n",
    "plot_training_history(history, \"Autoencoder\")\n",
    "\n",
    "# Plot regressor training history (after training best_regressor)\n",
    "regressor_history = best_regressor.fit(latent_train, y_train,\n",
    "                                      epochs=reg_epochs,\n",
    "                                      batch_size=32,\n",
    "                                      validation_split=0.1,\n",
    "                                      verbose=1)\n",
    "plot_training_history(regressor_history, \"Regressor\")\n",
    "\n",
    "# 2. Latent Space Visualization\n",
    "def visualize_latent_space(latent_rep, y, title):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # PCA Visualization\n",
    "    plt.subplot(1, 2, 1)\n",
    "    pca = PCA(n_components=2)\n",
    "    latent_pca = pca.fit_transform(latent_rep)\n",
    "    plt.scatter(latent_pca[:, 0], latent_pca[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title(f'{title} - PCA Projection')\n",
    "\n",
    "    # t-SNE Visualization\n",
    "    plt.subplot(1, 2, 2)\n",
    "    tsne = TSNE(n_components=2, perplexity=30)\n",
    "    latent_tsne = tsne.fit_transform(latent_rep)\n",
    "    plt.scatter(latent_tsne[:, 0], latent_tsne[:, 1], c=y, cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(label='Target Value')\n",
    "    plt.title(f'{title} - t-SNE Projection')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{title}_latent_space.png')\n",
    "    plt.show()\n",
    "\n",
    "visualize_latent_space(latent_train, y_train, \"Training_Latent_Space\")\n",
    "\n",
    "# 3. Prediction Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_inv, y_pred, alpha=0.6)\n",
    "plt.plot([min(y_test_inv), max(y_test_inv)],\n",
    "         [min(y_test_inv), max(y_test_inv)], 'r--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Actual vs Predicted Values (R² = {r2:.2f})')\n",
    "plt.grid(True)\n",
    "plt.savefig('actual_vs_predicted.png')\n",
    "plt.show()\n",
    "\n",
    "# 4. Residual Plot\n",
    "residuals = y_test_inv - y_pred\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.residplot(x=y_pred, y=residuals, lowess=True,\n",
    "             line_kws={'color': 'red', 'lw': 1})\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Analysis')\n",
    "plt.grid(True)\n",
    "plt.savefig('residual_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# # 5. Feature Importance (Permutation Importance)\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# result = permutation_importance(\n",
    "#     best_regressor, latent_test, y_test_inv,\n",
    "#     n_repeats=10, random_state=42,\n",
    "#     scoring='neg_mean_squared_error'\n",
    "# )\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.boxplot(result.importances[sorted_idx].T,\n",
    "#             vert=False, labels=np.arange(latent_train.shape[1])[sorted_idx])\n",
    "# plt.title(\"Permutation Importance (Latent Features)\")\n",
    "# plt.xlabel(\"Importance\")\n",
    "# plt.ylabel(\"Latent Feature Index\")\n",
    "# plt.savefig('feature_importance.png')\n",
    "# plt.show()\n",
    "\n",
    "# 6. Model Architecture Visualization\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Visualize autoencoder\n",
    "plot_model(best_model, to_file='autoencoder_architecture.png',\n",
    "          show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# Visualize regressor\n",
    "plot_model(best_regressor, to_file='regressor_architecture.png',\n",
    "          show_shapes=True, show_layer_names=True)\n",
    "\n",
    "# 7. Hyperparameter Optimization Results\n",
    "def plot_hpo_results(tuner):\n",
    "    results = tuner.oracle.get_best_trials(num_trials=10)\n",
    "    hp_names = list(tuner.get_best_hyperparameters()[0].values.keys())\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, hp_name in enumerate(hp_names):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        values = [t.hyperparameters.values[hp_name] for t in results]\n",
    "        losses = [t.score for t in results]\n",
    "        plt.scatter(values, losses)\n",
    "        plt.title(hp_name)\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Validation Loss')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('hpo_results.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_hpo_results(tuner)\n",
    "plot_hpo_results(tuner_regressor)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
